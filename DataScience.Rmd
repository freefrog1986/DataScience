---
title: "DataScience"
author: "LiuBo"
date: "6/24/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary
This is all about how to do data analysis using R.
steps:
1. Understanding the problem
        The first step in analyzing any new data set is figuring out: (a) what data you have and (b) what are the standard tools and models used for that type of data.
2. Data acquisition and cleaning
        getting data
3. Exploratory analysis
4. Statistical modeling
5. Predictive modeling
6. Creative exploration
7. Creating a data product
8. Creating a short slide deck pitching your product

## 1. Data
This section focus on dealing with data, including Loading Data, Cleaning Data.

### 1.1 Loading Data

#### load data locally
```{r load data}
# first set working directory
setwd('/Users/freefrog/Studing/DataScience/Practical_ML')
rm(list = ls()) # remove all variabls in the eviroment

# downloading files
url <- 'https://d396qusza40orc.cloudfront.net/exdata%2Fdata%2FNEI_data.zip'
localfile <- file.path(getwd(),'original_file')
download.file(url, localfile)
unzip(localfile)

# read .csv file
# !!! How to treat na values are very important!!!
Train_Data <- read.csv('pml-training.csv',na.strings=c("", "NA", "NULL"))
Test_Data <- read.csv('pml-testing.csv', na.strings=c("", "NA", "NULL"))

dim(Train_Data);dim(Test_Data)
```
### 1.2 Cleaning The Data
In this section, we will remove variables that   
- Have too many NA values.      
- Unrelated variables.  
- Low variance variables.
- Highly correlated variables.

#### Removing NAs
``` {r removing NAs}
# find all variables contain NA values
Train_na_variables <- which(colSums(is.na(Train_Data))==0)
Test_na_variables <- which(colSums(is.na(Test_Data))==0)

# or find variables which has more than 95% NA values
# Train_na_Variables <- sapply(Train_Data, function(x) mean(is.na(x))) > 0.95
# Test_na_Variables <- sapply(Test_Data, function(x) mean(is.na(x))) > 0.95

Training <- Train_Data[,Train_na_variables]
Testing <- Test_Data[,Test_na_variables]

dim(Training);dim(Testing)
```

#### Removing Uncorrelateed Variables
```{r removing uncorrelateed variables}
# find uncorrelated variables
remove = c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window')

Training <- Training[,-which(names(Training) %in% remove)]
Testing <- Testing[,-which(names(Testing) %in% remove)]

dim(Training);dim(Testing)
```

#### Removing Low Variance Variables
```{r removing low variance variables}
library(caret)
# using nearZeroVar to get those low variance variables, this only can apply on numeric formate.
zeroVar = nearZeroVar(Training[,sapply(Training, is.numeric)],
                     saveMetrics = TRUE)

Training = Training[,zeroVar[, 'nzv']==0]
Testing = Testing[,zeroVar[, 'nzv']==0]

dim(Training);dim(Testing)
```

#### Removing Highly Correlated Variables
```{r}
# using cor to get thee correlation of all numeric variables
corrMatrix <- cor(Training[sapply(Training, is.numeric)])
# set grid
corrDF <- expand.grid(row = 1:dim(corrMatrix)[1], 
                      col = 1:dim(corrMatrix)[2])
# set data to vector so that can be plot
corrDF$correlation <- as.vector(corrMatrix)
# using levelplot to show the relationship between all those variables.
levelplot(correlation ~ row+ col, corrDF)
# find high correlated variables( >90% )
High_cor_var = findCorrelation(corrMatrix, 
                   cutoff = .90, 
                   verbose = TRUE)

Training = Training[, -High_cor_var]
Testing = Testing[, -High_cor_var]

dim(Training);dim(Testing)
```
## 2. Analysis

### Machine Learning Algorithm

 #### Linear Discriminant Analysis
```{r}
Model_LDA <- train(classe ~ ., 
                   method="lda", 
                   preProcess="pca", 
                   data=Training)

Pred_LDA_Train <- predict(Model_LDA, newdata = Training)

Outcome_LDA_Train <- confusionMatrix(Pred_LDA_Train , Training$classe)
Outcome_LDA_Train$overall[1]
```

#### Naive Bayes
```{r}
library(klaR)
# preprocess befor fit the Naive Bayes method using pca method
Pre_Proc_Train <- preProcess(Training[,-46], method="pca", pcaComp=10)
TrainingPC <- predict(Pre_Proc_Train, Training[,-46])

Model_NB  <- NaiveBayes(Training$classe ~ ., data=TrainingPC)

Pred_NB_Train <- predict(Model_NB, newdata = TrainingPC)$class

Outcome_NB_Train <- confusionMatrix(Pred_NB_Train , Training$classe)
Outcome_NB_Train$overall[1]
```

#### Gradient Boosted Tree model
```{r}
library(caret)
Model_BoostTree <- train(classe ~ ., 
                         method="gbm", 
                         data=Training, 
                         verbose=FALSE)

Pred_BT_Train <- predict(Model_BoostTree, newdata = Training)

Outcome_BT_Train <- confusionMatrix(Pred_BT_Train , Training$classe)
Outcome_BT_Train$overall[1]
```

### Decision Tree Model
```{r fig.width=8, fig.height = 8}
library(rpart)
library(rpart.plot)
set.seed(12345)

modFitDT <- rpart(classe ~ ., data = Training_Data,
                  method="class",
                  control = rpart.control(method = "cv",
                                          number = 10))

rpart.plot(modFitDT, main = "Classification Tree")

Pre_Tree <- predict(modFitDT, Testing_Data, type = "class")
confusionMatrix(Pre_Tree, Testing_Data$classe)
```

## Building an R package

### Create a new directory with R/ and man/ sub-directory
`package.skeleton()`

### Write a DESCRIPTION file

### Copy R code into the R/ sub-directory

### Write R documentation files in man/ sub-directory

### Write a NAMESPACE file with exports/imports

### Build and Check








